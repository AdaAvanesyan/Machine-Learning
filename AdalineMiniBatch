class AdalineMiniBatchGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1, batch_size=16):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        self.batch_size = batch_size

    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])  # Adjusted for bias weight
        self.losses_ = []

        for _ in range(self.n_iter):
            losses = []
            for X_batch, y_batch in self.generate_batches(X, y):
                net_input = self.net_input(X_batch)
                output = self.activation(net_input)
                errors = (y_batch - output)
                self.w_[1:] += self.eta * X_batch.T.dot(errors)
                self.w_[0] += self.eta * errors.sum()
                loss = (errors**2).mean()
                losses.append(loss)
            avg_loss = np.mean(losses)
            self.losses_.append(avg_loss)
        return self

    def generate_batches(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        shuffled_indices = rgen.permutation(len(y))
        X_shuffled = X[shuffled_indices]
        y_shuffled = y[shuffled_indices]

        for start_idx in range(0, len(y), self.batch_size):
            end_idx = min(start_idx + self.batch_size, len(y))
            yield X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        return X

    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
    
    
    
ada_mini_batch = AdalineMiniBatchGD(n_iter=15, eta=0.01, batch_size=16).fit(X_std, y)
ada_mini_batch.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada_mini_batch)
plt.title('AdalineMiniBatchGD')
plt.xlabel('Sepal length [standardized]')
plt.ylabel('Petal length [standardized]')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()
