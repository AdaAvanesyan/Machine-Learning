import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np

random_state = 9210159 
np.random.seed(random_state)
X = np.random.randint(1, 10, size=(4,3))
y = np.asarray([1, 1, 0, 0]).reshape(-1, 1)
rgen = np.random.RandomState(random_state)
w = np.round(rgen.normal(loc=0.0, scale=0.1, size=X.shape[1]), 2)
b = np.float_(0.)
learning_rate = 0.2
print(f"""
Features:
{X}
Labels:
{y}
Initial weights:
{w}
Initial bias:
{b}
Learning rate:
{learning_rate}
""")


class AdalineGD:
    def __init__(self, eta=0.1, n_iter=50, random_state=1):
        self.eta=eta
        self.n_iter=n_iter
        self.random_state=random_state
      
        
    def fit(self, X, y):
        
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float_(0.)
        self.losses_ = []
        
        
        for _ in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)  # Update weights
            self.w_[0] += self.eta * errors.sum()  # Update bias
            loss = (errors**2).mean()
            self.losses_.append(loss)
        return self
        

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, X):
        return X
        
    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)

        

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

# Training a model with a relatively large learning rate
ada1 = AdalineGD(n_iter=15, eta=0.1).fit(X, y)
ax[0].plot(range(1, len(ada1.losses_) + 1), np.log10(ada1.losses_), marker='o')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('log(Mean squared error)')
ax[0].set_title('Adaline - Learning rate 0.1')

# Tranining a model with a very small learning rate
ada2 = AdalineGD(n_iter=15, eta=0.0001).fit(X, y)
ax[1].plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Mean squared error')
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()
